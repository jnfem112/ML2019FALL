\documentclass{article}

\usepackage{enumitem}
\setenumerate[1]{itemsep = 1pt , partopsep = 1pt , parsep = \parskip , topsep = 1pt}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xeCJK}
\setCJKmainfont[AutoFakeBold = 2.5 , AutoFakeSlant = 0.4]{cwTeXKai}
\newcommand{\parallelsum}{\mathbin{\!/\mkern-5mu/\!}}

\title{Machine Learning - Homework 2}
\author{資工四 B05902023 李澤諺}
\date{October 26, 2019}

\begin{document}

\maketitle

\noindent
{\bf \LARGE Part 1. Programming Problem}\\

\noindent
{\bf 1. (0.5\%)請比較你實作的generative model、logistic regression的準確率，何者較佳?}\\

下表為我實作中performance最好的generative model和logistic regression分別在training和testing時所得到的accuracy：

\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multirow{2}*{} & \multirow{2}*{Train} & \multirow{2}*{Validation} & \multicolumn{2}{|c|}{Test}\\
        \cline{4-5}
        & & & Public & Private\\
        \hline
        Generative Model & 0.84842 & 0.83100 & 0.85442 & 0.85112\\
        \hline
        Logistic Regression & 0.85856 & 0.84000 & 0.85995 & 0.85444\\
        \hline
    \end{tabular}
\end{center}

其中，我隨機選出1000筆data作為validation data，剩下的則作為training data，而我實作中performance最好的generative model為：先將age、fnlwgt、capital\_gain、capital\_loss、hours\_per\_week 這5個feature的值提升至2到3次方，並將所有feature進行normalization後，再建立generative model，而我實作中performance最好的logistic regression為：先將age、fnlwgt、capital\_gain、capital\_loss、hours\_per\_week這5個feature的值提升至2到10次方，並將所有feature進行normalization後，
再使用助教提供的logistic regression函式(除了epoch改為300以外，其餘hyper-parameter皆不變)而得。

由上表可以看出我實作的model中，logistic regression的performance較generative model好。\\

\noindent
{\bf 2. (0.5\%)請實作特徵標準化(feature normalization)並討論其對於你的模型準
確率的影響。}\\

下表為我實作的generative model(方法如第1題所述)，有使用normalization和沒有使用normalization分別在training和testing時所得到的accuracy：

\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multirow{2}*{} & \multirow{2}*{Train} & \multirow{2}*{Validation} & \multicolumn{2}{|c|}{Test}\\
        \cline{4-5}
        & & & Public & Private\\
        \hline
        Normalization & 0.84842 & 0.83100 & 0.85442 & 0.85112\\
        \hline
        Without Normalization & 0.84730 & 0.83000 & 0.85429 & 0.85038\\
        \hline
    \end{tabular}
\end{center}

下表為我實作的logistic regression(方法如第1題所述)，有使用normalization和沒有使用normalization分別在training和testing時所得到的accuracy：

\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multirow{2}*{} & \multirow{2}*{Train} & \multirow{2}*{Validation} & \multicolumn{2}{|c|}{Test}\\
        \cline{4-5}
        & & & Public & Private\\
        \hline
        Normalization & 0.85856 & 0.84000 & 0.85995 & 0.85444\\
        \hline
        Without Normalization & 0.76321 & 0.75200 & 0.76928 & 0.76722\\
        \hline
    \end{tabular}
\end{center}

下表為我實作的best model(方法如第3題所述)，有使用normalization和沒有使用normalization分別在training和testing時所得到的accuracy：

\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \multirow{2}*{} & \multirow{2}*{Train} & \multirow{2}*{Validation} & \multicolumn{2}{|c|}{Test}\\
        \cline{4-5}
        & & & Public & Private\\
        \hline
        Normalization & 0.86746 & 0.86800 & 0.87137 & 0.86561\\
        \hline
        Without Normalization & 0.86724 & 0.86400 & 0.87100 & 0.86524\\
        \hline
    \end{tabular}
\end{center}

由此可以大致看出，有使用normalization可以使得model的performance較好。\\

\noindent
{\bf 3. (1\%) 請說明你實作的best model，其訓練方式和準確率為何?}\\

我將data進行normalization之後，隨機選出1000筆作為validation data，剩下的則作為training data，以此訓練sklearn的GradientBoostingClassifier，其中，model的parameter如下：

\begin{center}
    \begin{tabular}{|c|c|}
        \hline
        loss & 'deviance'\\
        \hline
        learning\_rate & 0.1\\
        \hline
        n\_estimators & 100\\
        \hline
        validation\_fraction & 0.1\\
        \hline
        n\_iter\_no\_change & 10\\
        \hline
        tol & 0.0001\\
        \hline
    \end{tabular}
\end{center}

最後所得到的accuracy如下表所示：

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \multirow{2}*{Train} & \multirow{2}*{Validation} & \multicolumn{2}{|c|}{Test}\\
        \cline{3-4}
        & & Public & Private\\
        \hline
        0.86746 & 0.86800 & 0.87137 & 0.86561\\
        \hline
    \end{tabular}
\end{center}

\bigskip

\noindent
{\bf \LARGE Part 2. Math Problem}\\

\noindent
{\bf 1.}\\

Consider a generative classification model for K classes defined by prior class probabilities $p(C_k) = \pi_k$ and general class-conditional densities $p(x|C_k)$, where x is the input feature vector. Suppose we are given a training data set $\{x_n, t_n\}$ where $n = 1,\ \cdots,\ N$, and $t_n$ is a binary target vector of length K that uses the 1−of−K coding scheme, so that it has components $t_{nk} = 1$ if pattern n is from class $C_k$, otherwise $t_{nk} = 0$. Assuming that the data points are drawn independently from this model, show that the maximum-likelihood solution for the prior probabilities is given by
\begin{align*}
    \pi_k = \frac{N_k}{N}
\end{align*}
where $N_k$ is the number of data points assigned to class $C_k$.\\

\noindent
{\bf \large solution}\\

\noindent
令 $x_n$ 所屬的class為 $C_{x_n}$。\\
因為likelihood function為
\begin{align*}
    P(x_1, x_2, \cdots, x_N) = \prod_{n = 1}^N P(x_n) = \prod_{n = 1}^N P(C_{x_n}) P(x_n | C_{x_n})
\end{align*}
將上式取log，可得log likelihood function為
\begin{align*}
    log P(x_1, x_2, \cdots, x_N) &= \sum_{n = 1}^N log P(C_{x_n}) + \sum_{n = 1}^N log P(x_n | C_{x_n})\\
    &= \sum_{k = 1}^K N_k log P(C_k) + \sum_{n = 1}^N log P(x_n | C_{x_n})\\
    &= \sum_{k = 1}^K N_k log \pi_k + \sum_{n = 1}^N log P(x_n | C_{x_n})
\end{align*}
只要log likelihood function有最大值，即可使得likelihood function亦有最大值，此外，注意有限制條件 $\sum_{k = 1}^K \pi_k = 1$。\\
因此，可以試著使用Lagrange multiplier，在限制條件 $\sum_{k = 1}^K \pi_k = 1$ 下，求出log likelihood function的最大值。\\
令 $f = log P(x_1, x_2, \cdots, x_N)$，以及 $g = \sum_{k = 1}^K \pi_k = 1$。\\
因為
\begin{align*}
    \frac{\partial}{\partial \pi_i} f &= \frac{\partial}{\partial \pi_i} (\sum_{k = 1}^K N_k log \pi_k + \sum_{n = 1}^N log P(x_n | C_{x_n}))\\
    &= \frac{\partial}{\partial \pi_i} \sum_{k = 1}^K N_k log \pi_k + 0\\
    &= \frac{\partial}{\partial \pi_i} N_i log \pi_i = \frac{N_i}{\pi_i}
\end{align*}
所以
\begin{align*}
    \nabla f = 
    \left(
    \begin{array}{c}
        \frac{\partial}{\partial \pi_1} f\\
        \frac{\partial}{\partial \pi_2} f\\
        \vdots\\
        \frac{\partial}{\partial \pi_K} f
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        \frac{N_1}{\pi_1}\\
        \frac{N_2}{\pi_2}\\
        \vdots\\
        \frac{N_K}{\pi_K}\\
    \end{array}
    \right)
\end{align*}
而
\begin{align*}
    \frac{\partial}{\partial \pi_i} g = \frac{\partial}{\partial \pi_i} \sum_{k = 1}^K \pi_k = \frac{\partial}{\partial \pi_i} \pi_i = 1
\end{align*}
所以
\begin{align*}
    \nabla g = 
    \left(
    \begin{array}{c}
        \frac{\partial}{\partial \pi_1} g\\
        \frac{\partial}{\partial \pi_2} g\\
        \vdots\\
        \frac{\partial}{\partial \pi_K} g
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        1\\
        1\\
        \vdots\\
        1\\
    \end{array}
    \right)
\end{align*}
因此，若令 $\nabla f = \lambda \nabla g$，則有
\begin{align*}
    \left(
    \begin{array}{c}
        \frac{N_1}{\pi_1}\\
        \frac{N_2}{\pi_2}\\
        \vdots\\
        \frac{N_K}{\pi_K}\\
    \end{array}
    \right)
    = \lambda
    \left(
    \begin{array}{c}
        1\\
        1\\
        \vdots\\
        1\\
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
        \lambda\\
        \lambda\\
        \vdots\\
        \lambda\\
    \end{array}
    \right)
\end{align*}
所以 $\pi_i = \frac{N_i}{\lambda}$，又
\begin{align*}
    \sum_{k = 1}^K \pi_k = \sum_{k = 1}^K \frac{N_k}{\lambda} = \frac{N}{\lambda} = 1
\end{align*}
因此可得 $\lambda = N$，故
\begin{align*}
    \pi_i = \frac{N_i}{\lambda} = \frac{N_i}{N}\ \square
\end{align*}

\noindent
{\bf 2.}\\

Show that
\begin{align*}
    \frac{\partial log(det \Sigma)}{\partial \sigma_{ij}} = e_j \Sigma^{−1} e^T_i
\end{align*}
where $\Sigma \in {\mathbb R}^{m \times m}$ is a (non-singular) covariance matrix and $e_j$ is a row vector(ex: $e_3 = [0, 0, 1, 0, \cdots, 0]$).\\

\noindent
{\bf \large solution}\\

\noindent
令 $\Sigma$ 的$(p, q)$ cofactor為 $C_{pq}$。\\
\begin{align*}
    \frac{\partial}{\partial \sigma_{ij}} log\ det \Sigma &= \frac{1}{det \Sigma} \frac{\partial}{\partial \sigma_{ij}} det \Sigma\\
    &= \frac{1}{det \Sigma} \frac{\partial}{\partial \sigma_{ij}} (\sigma_{i1} C_{i1} + \sigma_{i2} C_{i2} + \cdots + \sigma_{im} C_{im})\\
    &= \frac{1}{det \Sigma} C_{ij} = \frac{1}{det \Sigma} (adj \Sigma)_{ji} =  (\frac{1}{det \Sigma} adj \Sigma)_{ji}\\
    &= (\Sigma^{-1})_{ji} = e_j \Sigma^{-1} e_i^T\ \square
\end{align*}

\noindent
{\bf 3.}\\

Consider the classification model of problem 1 and result of problem 2 and now suppose that the class-condition densities are given by Gaussian distributions with a shared convariance matrix, so that
\begin{align*}
    p(x|C_k) = {\mathcal N}(x|\mu_k, \Sigma)
\end{align*}

Show that the maximum likelihood solution for the mean of the Gaussian distribution for class $C_k$ is given by
\begin{align*}
    \mu_k = \frac{1}{N_k} \sum_{n = 1}^{N} t_{nk}x_n
\end{align*}
which represents the mean of those feature vectors assigned to class $C_k$.\\

Similarly, show that the maximum likelihood solution for the shared covariance matrix is given by
\begin{align*}
    \Sigma = \sum_{k = 1}^{K} \frac{N_k}{N} S_k
\end{align*}
where
\begin{align*}
    S_k = \frac{1}{N_k} \sum_{n = 1}^{N} t_{nk}(x_n − \mu_k)(x_n − \mu_k)^T
\end{align*}

Thus $\Sigma$ is given by a weighted average of the covariance of the data associated with each class, in which the weighting coefficients are given by the prior probabilities of the classes.\\

\noindent
{\bf \large solution}\\

\noindent
由第1題可知，只要log likelihood function有最大值，即可使得likelihood function亦有最大值，而log likelihood function為
\begin{align*}
    log P(x_1, x_2, \cdots, x_N) &= \sum_{k = 1}^K N_k log \pi_k + \sum_{n = 1}^N log P(x_n | C_{x_n})\\
    &= \sum_{k = 1}^K N_k log \pi_k + \sum_{k = 1}^K \sum_{x \in C_k} log P(x_n | C_k)\\
    &= \sum_{k = 1}^K N_k log \pi_k + \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log P(x_n | C_k)\\
    &= \sum_{k = 1}^K N_k log \pi_k + \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log {\mathcal N}(x_n|\mu_k, \Sigma)
\end{align*}
其中，僅有 $\sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log {\mathcal N}(x_n|\mu_k, \Sigma)$ 和 $\mu_1$、$\mu_2$、$\cdots$、$\mu_K$、$\Sigma$ 有關，因此，只要求出 $\mu_1$、$\mu_2$、$\cdots$、$\mu_K$、$\Sigma$ 使得 $\sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log {\mathcal N}(x_n|\mu_k, \Sigma)$ 有最大值，該 $\mu_1$、$\mu_2$、$\cdots$、$\mu_K$、$\Sigma$ 即可使得log likelihood function有最大值。\\
令
\begin{align*}
    l &= \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log {\mathcal N}(x_n|\mu_k, \Sigma)\\
    &= \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} log(\frac{1}{\sqrt{(2 \pi)^m det \Sigma}} e^{-\frac{1}{2} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n)})\\
    &= \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} (-\frac{1}{2} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi)
\end{align*}
因為
\begin{align*}
    \frac{\partial l}{\partial \mu_i} &= \frac{\partial}{\partial \mu_i} \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} (-\frac{1}{2} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi)\\
    &= \frac{\partial}{\partial \mu_i} \sum_{n = 1}^N t_{ni} (-\frac{1}{2} (\mu_i - x_n)^T \Sigma^{-1} (\mu_i - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi)\\
    &= \sum_{n = 1}^N (t_{ni} \cdot \frac{\partial}{\partial \mu_i} (-\frac{1}{2} (\mu_i - x_n)^T \Sigma^{-1} (\mu_i - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi))\\
    &= \sum_{n = 1}^N t_{ni} (-\frac{1}{2} \cdot 2 \Sigma^{-1} (\mu_i - x_n)) = \sum_{n = 1}^N (\Sigma^{-1}((-t_{ni})(\mu_i - x_n)))\\
    &= \Sigma^{-1} \cdot \sum_{n = 1}^N (t_{ni} x_n - t_{ni} \mu_i) = \Sigma^{-1} \cdot (\sum_{n = 1}^N t_{ni} x_n - \sum_{n = 1}^N t_{ni} \mu_i)\\
    &= \Sigma^{-1} \cdot (\sum_{n = 1}^N t_{ni} x_n - (\sum_{n = 1}^N t_{ni}) \mu_i) = \Sigma^{-1} \cdot (\sum_{n = 1}^N t_{ni} x_n - N_i \mu_i)\\
\end{align*}
因此，令$\frac{\partial l}{\partial \mu_i} = 0$，可得
\begin{gather*}
    \Sigma^{-1} \cdot (\sum_{n = 1}^N t_{ni} x_n - N_i \mu_i) = 0\\
    \sum_{n = 1}^N t_{ni} x_n - N_i \mu_i = 0\\
    \mu_i = \frac{1}{N_i} \sum_{n = 1}^N t_{ni} x_n
\end{gather*}
接著，由於第2題的證明中並未使用到任何covariance matrix的性質，因此事實上由第2題的證明，可得：$\forall\ A \in {\mathbb R}^{m \times m}$，若 $A$ 為invertible，則有 $\frac{\partial}{\partial A_{ij}} log\ det A = (A^{-1})_{ji}$，故 $\frac{\partial}{\partial A} log\ det A = (A^{-1})^T$。\\
因此，當 $\Sigma$ 為covariance matrix且為invertible時，可得
\begin{align*}
    \frac{\partial}{\partial \Sigma^{-1}} log\ det \Sigma &= \frac{\partial}{\partial \Sigma^{-1}} log\ \frac{1}{det \Sigma^{-1}}\\
    &= -\frac{\partial}{\partial \Sigma^{-1}} log\ det \Sigma^{-1}\\
    &= -((\Sigma^{-1})^{-1})^T = -\Sigma^T = -\Sigma
\end{align*}
此外
\begin{align*}
    \frac{\partial}{\partial \Sigma^{-1}} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n) = (\mu_k - x_n) (\mu_k - x_n)^T
\end{align*}
所以
\begin{align*}
    \frac{\partial l}{\partial \Sigma^{-1}} &= \frac{\partial}{\partial \Sigma^{-1}} \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} (-\frac{1}{2} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi)\\
    &= \sum_{k = 1}^K \sum_{n = 1}^N (t_{nk} \cdot \frac{\partial}{\partial \Sigma^{-1}} (-\frac{1}{2} (\mu_k - x_n)^T \Sigma^{-1} (\mu_k - x_n) - \frac{1}{2} log\ det \Sigma - \frac{m}{2} log 2 \pi))\\
    &= \sum_{k = 1}^K \sum_{n = 1}^N t_{nk} (-\frac{1}{2} (\mu_k - x_n) (\mu_k - x_n)^T - \frac{1}{2} (-\Sigma))\\
    &= \frac{1}{2} \sum_{k = 1}^K \sum_{n = 1}^N (t_{nk} \Sigma - t_{nk} (\mu_k - x_n) (\mu_k - x_n)^T)\\
    &= \frac{1}{2} \sum_{k = 1}^K (\sum_{n = 1}^N t_{nk} \Sigma - \sum_{n = 1}^N t_{nk} (\mu_k - x_n) (\mu_k - x_n)^T)\\
    &= \frac{1}{2} \sum_{k = 1}^K ((\sum_{n = 1}^N t_{nk}) \Sigma - N_k S_k) = \frac{1}{2} \sum_{k = 1}^K (N_k \Sigma - N_k S_k)\\
    &= \frac{1}{2} (\sum_{k = 1}^K N_k \Sigma - \sum_{k = 1}^K N_k S_k) = \frac{1}{2} (N \Sigma - \sum_{k = 1}^K N_k S_k)
\end{align*}
因此，令 $\frac{\partial l}{\partial \Sigma^{-1}} = 0$，可得
\begin{gather*}
    \frac{1}{2} (N \Sigma - \sum_{k = 1}^K N_k S_k) = 0\\
    \Sigma = \frac{1}{N} \sum_{k = 1}^K N_k S_k = \sum_{k = 1}^K \frac{N_k}{N} S_k\ \square
\end{gather*}

\end{document}