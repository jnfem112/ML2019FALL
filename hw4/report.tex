\documentclass{article}

\usepackage{enumitem}
\setenumerate[1]{itemsep = 1pt , partopsep = 1pt , parsep = \parskip , topsep = 1pt}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xeCJK}
\setCJKmainfont[AutoFakeBold = 2.5 , AutoFakeSlant = 0.4]{cwTeXKai}
\newcommand{\parallelsum}{\mathbin{\!/\mkern-5mu/\!}}
\usepackage{hyperref}
\hypersetup{colorlinks = true , linkcolor = blue , filecolor = magenta , urlcolor = blue}
\usepackage{graphicx}
\graphicspath{{image/}}
\usepackage{stmaryrd}

\title{Machine Learning - Homework 4}
\author{資工四 B05902023 李澤諺}
\date{November 22, 2019}

\begin{document}

\maketitle

\noindent
{\bf \LARGE Part 1. Programming Problem}\\

\noindent
{\bf 1. (1\%) 請使用不同的Autoencoder model，以及不同的降維方式(降到不同維度)，討論其reconstruction loss和public/private accuracy。(因此模型需要兩種，降維方法也需要兩種，但clustering不用兩種。)}\\

以下為我於本題中所比較的兩個Autoencoder的架構：

\begin{center}
    \includegraphics[width=\textwidth]{Autoencoder_1.png}\\
\end{center}

\begin{center}
    \begin{tabular}{|c|l|}
        \hline
        \multicolumn{2}{|c|}{Autoencoder 1}\\
        \hline
        \multirow{2}*[-0.5em]{Encoder} & \makecell[l]{Conv2d(3 , 8 , kernel\_size = (3 , 3) , stride = (2 , 2) , \\padding = (1 , 1))}\\
        \cline{2-2}
        & \makecell[l]{Conv2d(8 , 16 , kernel\_size = (3 , 3) , stride = (2 , 2) , \\padding = (1 , 1))}\\
        \hline
        \multirow{3}*[-0.5em]{Decoder} & \makecell[l]{ConvTranspose2d(16 , 8 , kernel\_size = (2 , 2) , \\stride = (2 , 2))}\\
        \cline{2-2}
        & \makecell[l]{ConvTranspose2d(8 , 3 , kernel\_size = (2 , 2) , \\stride = (2 , 2))}\\
        \cline{2-2}
        & Tanh()\\
        \hline
    \end{tabular}
\end{center}

\begin{center}
    \includegraphics[width=\textwidth]{Autoencoder_2.png}\\
\end{center}

\begin{center}
    \begin{tabular}{|c|l|}
        \hline
        \multicolumn{2}{|c|}{Autoencoder 2}\\
        \hline
        \multirow{8}*[-1.75em]{Encoder} & \makecell[l]{Conv2d(3 , 1024 , kernel\_size = (3 , 3) , stride = (1 , 1) , \\padding = (1 , 1))}\\
        \cline{2-2}
        & MaxPool2d(2 , return\_indices = True)\\
        \cline{2-2}
        & \makecell[l]{Conv2d(1024 , 256 , kernel\_size = (3 , 3) , stride = (1 , 1) , \\padding = (1 , 1))}\\
        \cline{2-2}
        & MaxPool2d(2 , return\_indices = True)\\
        \cline{2-2}
        & \makecell[l]{Conv2d(256 , 64 , kernel\_size = (3 , 3) , stride = (1 , 1) , \\padding = (1 , 1))}\\
        \cline{2-2}
        & MaxPool2d(2 , return\_indices = True)\\
        \cline{2-2}
        & \makecell[l]{Conv2d(64 , 16 , kernel\_size = (3 , 3) , stride = (1 , 1) , \\padding = (1 , 1))}\\
        \cline{2-2}
        & MaxPool2d(2 , return\_indices = True)\\
        \hline
        \multirow{9}*[-2.25em]{Decoder} & MaxUnpool2d(2)\\
        \cline{2-2}
        & \makecell[l]{ConvTranspose2d(16 , 64 , kernel\_size = (3 , 3) , \\stride = (1 , 1) , padding = (1 , 1))}\\
        \cline{2-2}
        & MaxUnpool2d(2)\\
        \cline{2-2}
        & \makecell[l]{ConvTranspose2d(64 , 256 , kernel\_size = (3 , 3) , \\stride = (1 , 1) , padding = (1 , 1))}\\
        \cline{2-2}
        & MaxUnpool2d(2)\\
        \cline{2-2}
        & \makecell[l]{ConvTranspose2d(256 , 1024 , kernel\_size = (3 , 3) , \\stride = (1 , 1) , padding = (1 , 1))}\\
        \cline{2-2}
        & MaxUnpool2d(2)\\
        \cline{2-2}
        & \makecell[l]{ConvTranspose2d(1024 , 3 , kernel\_size = (3 , 3) , \\stride = (1 , 1) , padding = (1 , 1))}\\
        \cline{2-2}
        & Tanh()\\
        \hline
    \end{tabular}
\end{center}

\bigskip

兩個Autoencoder皆為使用Adam訓練，learning rate為0.0001，使用l1-loss作為loss function，batch size為256，訓練了20個epoch。\\

Autoencoder 1會將圖片降到1024維，而Autoencoder 2會將圖片降到64維，在兩個Autoencoder分別做完第一次的降維之後，接著皆會使用t-SNE進行第二次的降維，將圖片降到2維，最後使用K-Means進行clustering。\\

以下為兩種方法分別所得到的average reconstruction loss以及public/private accuracy：

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        & reconstruction loss & public accuracy & private accuracy\\
        \hline
        Autoencoder 1 & 0.00084342 & 0.69259 & 0.70063\\
        \hline
        Autoencoder 2 & 0.00031634 & 0.75333 & 0.75920\\
        \hline
    \end{tabular}
\end{center}

\bigskip

\noindent
{\bf 2. (1\%) 從dataset選出2張圖，並貼上原圖以及經過autoencoder後reconstruct的圖片。}\\

以下為從training dataset中取出前2張圖，使用第1題訓練出來的Autoencoder 2，將圖片進行降維並reconstruct之後的結果：

\begin{center}
    \includegraphics[width=\textwidth]{reconstruction.png}\\
\end{center}

\noindent
{\bf 3. (1\%) 在之後我們會給你dataset的label。請在二維平面上視覺化label的分佈。}\\

以下為使用第1題訓練出來的Autoencoder 2以及t-SNE，將圖片進行降維之後的結果：

\begin{center}
    \includegraphics[width=\textwidth]{scatter.png}\\
\end{center}

\bigskip

\noindent
{\bf \LARGE Part 2. Math Problem}\\

In this problem set, we denote $\llbracket a, b \rrbracket = \{i \in {\mathbb Z} : a \le i \le b\}$.\\

\bigskip

\noindent
{\bf \Large 1. Principle Component Analysis (1\%)}\\

Given 10 samples in 3D: $(1, 2, 3), (4, 8, 5), (3, 12, 9), (1, 8, 5), (5, 14, 2),
(7, 4, 1),$\\
$(9, 8, 9), (3, 8, 1), (11, 5, 6), (10, 11, 7)$.\\

\begin{enumerate}
    \item[(a)] What are the principal axes?
    \item[(b)] Please compute the principal components for each sample.
    \item[(c)] What is the average reconstruction error if reduce dimension to 2D? Here the reconstruction error is defined as the squared loss.\\
\end{enumerate}

\noindent
{\bf \large solution}\\

\noindent
{\bf (a)}\\

\noindent
令題幹中給定的點依序為 $x_1$、$x_2$、$\cdots$、$x_{10} \in {\mathbb R}^3$。\\
$x_1$、$x_2$、$\cdots$、$x_{10}$ 的mean和covariance matrix分別為
\begin{gather*}
    \mu = \frac{1}{10} \sum_{n = 1}^{10} x_n = 
    \left(
    \begin{array}{c}
        5.4\\
        8\\
        4.8
    \end{array}
    \right)\\
    \Sigma = \frac{1}{10} \sum_{n = 1}^{10} (x_n - \mu)(x_n - \mu)^T = 
    \left(
    \begin{array}{ccc}
        12.04 & 0.5 & 3.28\\
        0.5 & 12.2 & 2.9\\
        3.28 & 2.9 & 8.16
    \end{array}
    \right)
\end{gather*}
將 $\Sigma$ 正交對角化為 $\Sigma = Q \Lambda Q^T$，其中
\begin{gather*}
    Q = 
    \left(
    \begin{array}{ccc}
        0.616596 & 0.678179 & -0.399856\\
        0.58815 & -0.73439 & -0.337589\\
        0.522596 & 0.0272856 & 0.852144
    \end{array}
    \right)\\
    \Lambda = 
    \left(
    \begin{array}{ccc}
        15.2974 & 0 & 0\\
        0 & 11.6305 & 0\\
        0 & 0 & 5.47203
    \end{array}
    \right)
\end{gather*}
因此可得 $\Sigma$ 的eigenvector為(依照eigenvalue的大小排序)
\begin{align*}
    v_1 = \left(
    \begin{array}{c}
        0.616596\\
        0.58815\\
        0.522596
    \end{array}
    \right)\ 
    v_2 = \left(
    \begin{array}{c}
        0.678179\\
        -0.73439\\
        0.0272856
    \end{array}
    \right)\ 
    v_3 = \left(
    \begin{array}{c}
        -0.399856\\
        -0.337589\\
        0.852144
    \end{array}
    \right)
\end{align*}
此即為principal axis。$\ \square$\\

\noindent
{\bf (b)}\\

\noindent
令
\begin{align*}
    W = \left(
    \begin{array}{c}
        v_1^T\\
        v_2^T\\
        v_3^T
    \end{array}
    \right)
    = \left(
    \begin{array}{ccc}
        0.616596 & 0.58815 & 0.522596\\
        0.678179 & -0.73439 & 0.0272856\\
        -0.399856 & -0.337589 & 0.852144
    \end{array}
    \right)
\end{align*}
則 $x_1$、$x_2$、$\cdots$、$x_{10}$ 的principal component依序為
\begin{align*}
    \begin{array}{ll}
        W x_1 = \left(
        \begin{array}{c}
            3.360684\\
            -0.7087442\\
            1.481398
        \end{array}
        \right) &
        W x_2 = \left(
        \begin{array}{c}
            9.784564\\
            -3.025976\\
            -0.039416
        \end{array}
        \right)\\
        & \\
        W x_3 = \left(
        \begin{array}{c}
            13.610952\\
            -6.5365726\\
            2.41866
        \end{array}
        \right) & 
        W x_4 = \left(
        \begin{array}{c}
            7.934776\\
            -5.060513\\
            1.160152
        \end{array}
        \right)\\
        & \\
        W x_5 = \left(
        \begin{array}{c}
            12.362272\\
            -6.8359938\\
            -5.021238
        \end{array}
        \right) & 
        W x_6 = \left(
        \begin{array}{c}
            7.191368\\
            1.8369786\\
            -3.297204
        \end{array}
        \right)\\
        & \\
        W x_7 = \left(
        \begin{array}{c}
            14.957928\\
            0.4740614\\
            1.36988
        \end{array}
        \right) & 
        W x_8 = \left(
        \begin{array}{c}
            7.077584\\
            -3.8132974\\
            -3.048136
        \end{array}
        \right)\\
        & \\
        W x_9 = \left(
        \begin{array}{c}
            12.858882\\
            3.9517326\\
            -0.973497
        \end{array}
        \right) & 
        W x_{10} = \left(
        \begin{array}{c}
            16.293782\\
            -1.105508\\
            -1.747031
        \end{array}
        \right)\ \square
    \end{array}
\end{align*}

\noindent
{\bf (c)}\\

\noindent
令
\begin{align*}
    {\tilde W} = \left(
    \begin{array}{c}
        v_1^T\\
        v_2^T
    \end{array}
    \right)
    = \left(
    \begin{array}{ccc}
        0.616596 & 0.58815 & 0.522596\\
        0.678179 & -0.73439 & 0.0272856
    \end{array}
    \right)
\end{align*}
則average reconstruction error為
\begin{align*}
    \frac{1}{10} \sum_{n = 1}^{10} \| x_n - {\tilde W}^T ({\tilde W} x_n) \|^2 = 6.0681663\ \square
\end{align*}

\bigskip

\noindent
{\bf \Large 2. Constrained Mahalanobis Distance Minimization Problem (1\%)}\\

\begin{enumerate}
    \item[(a)] (0.25\%) Let $A \in {\mathbb R}^{m \times n}$, show that $AA^T$ and $A^TA$ are both symmetric and positive semi-definite, and share the same non-zero eigenvalues.
    \item[(b)] (0.25\%) Let $\Sigma \in {\mathbb R}^{m \times m}$ be a symmetric positive semi-definite matrix, $\mu \in {\mathbb R}^m$. Please construct a set of points $x_1, \cdots, x_n \in {\mathbb R}^m$ such that
    \begin{align*}
        \frac{1}{N} \sum_{i = 1}^{N} (x_i - \mu)(x_i - \mu)^T = \Sigma,\ \frac{1}{N} \sum_{i = 1}^{N} x_i = \mu
    \end{align*}
    \item[(c)] (0.5\%) Let $1 \le k \le m$, solve the following optimization problem (and justify with proof):
    \begin{align*}
        &minimize\ Trace(\Phi^T \Sigma \Phi)\\
        &subject\ to\ \Phi^T \Phi = I_k\\
        &variables\ \Phi \in {\mathbb R}^{m \times k}
    \end{align*}
\end{enumerate}

\noindent
{\bf \large solution}\\

\noindent
{\bf (a)}\\

\noindent
因為
\begin{align*}
    &(A A^T)^T = (A^T)^T A^T = A A^T\\
    &(A^T A)^T = A^T (A^T)^T = A^T A
\end{align*}
所以 $A A^T$ 和 $A^T A$ 皆為symmetric。\\
因為 $\forall\ x \in {\mathbb R}^m$ 且 $x \ne 0$，以及 $\forall\ y \in {\mathbb R}^n$ 且 $y \ne 0$，皆有
\begin{align*}
    &x^T (A A^T) x = (x^T A)(A^T x) = (A^T x)^T (A^T x) = \| A^T x \|^2 \ge 0\\
    &y^T (A^T A) y = (y^T A^T)(A y) = (A y)^T (A y) = \| A y \|^2 \ge 0
\end{align*}
所以 $A A^T$ 和 $A^T A$ 皆為positive semi-definite。\\\
令 $\lambda \ne 0$ 為 $A A^T$ 的一個eigenvalue。\\
取 $v \in {\mathbb R}^m$ 為其對應的一個eigenvector，則有
\begin{align*}
    (A A^T) v = \lambda v
\end{align*}
因此可得
\begin{align*}
    (A^T A) (A^T v) = A^T ((A A^T) v) = A^T (\lambda v) = \lambda (A^T v)
\end{align*}
故 $\lambda$ 亦為 $A^T A$ 的一個eigenvalue，而 $A^T v$ 為其對應的一個eigenvector。\\
同理，令 $\mu \ne 0$ 為 $A^T A$ 的一個eigenvalue。\\
取 $u \in {\mathbb R}^n$ 為其對應的一個eigenvector，則有
\begin{align*}
    (A^T A) u = \mu u
\end{align*}
因此可得
\begin{align*}
    (A A^T) (A v) = A ((A^T A) u) = A (\mu u) = \mu (A u)
\end{align*}
故 $\mu$ 亦為 $A A^T$ 的一個eigenvalue，而 $A u$ 為其對應的一個eigenvector。\\
由上述可得，$A A^T$ 和 $A^T A$ 有相同的non-zero eigenvalue。$\ \square$\\

\noindent
{\bf (b)}\\

\noindent
首先，取 $z_1$、$z_2$、$\cdots$、$z_{2m} \in {\mathbb R}^m$ 依序為\\
\begin{center}
    $\left(
    \begin{array}{c}
        \sqrt{m}\\
        0\\
        \vdots\\
        0
    \end{array}
    \right)\ 
    \left(
    \begin{array}{c}
        -\sqrt{m}\\
        0\\
        \vdots\\
        0
    \end{array}
    \right)\ 
    \left(
    \begin{array}{c}
        0\\
        \sqrt{m}\\
        \vdots\\
        0
    \end{array}
    \right)\ 
    \left(
    \begin{array}{c}
        0\\
        -\sqrt{m}\\
        \vdots\\
        0
    \end{array}
    \right)\ \cdots\ 
    \left(
    \begin{array}{c}
        0\\
        0\\
        \vdots\\
        \sqrt{m}
    \end{array}
    \right)\ 
    \left(
    \begin{array}{c}
        0\\
        0\\
        \vdots\\
        -\sqrt{m}
    \end{array}
    \right)$
\end{center}
則 $z_1$、$z_2$、$\cdots$、$z_{2m}$ 的mean為
\begin{align*}
    \frac{1}{2m} \sum_{k = 1}^{2m} z_k = 0
\end{align*}
而covariance matrix為
\begin{align*}
    \frac{1}{2m} \sum_{k = 1}^{2m} (z_k - 0)(z_k - 0)^T = \frac{1}{2m} \sum_{k = 1}^{2m} z_k z_k^T = I_m
\end{align*}
接著，因為 $\Sigma \in {\mathbb R}^{m \times m}$ 為positive semi-definite，所以 $\exists\ A \in {\mathbb R}^{m \times m}$ ，使得 $\Sigma = A A^T$ (例如可取 $\Sigma = A A^T$ 為 $\Sigma$ 的 Cholesky decomposition)。\\
取 $x_k = A z_k + \mu$，即可得到 $x_1$、$x_2$、$\cdots$、$x_{2m}$ 的mean為
\begin{align*}
    \frac{1}{2m} \sum_{k = 1}^{2m} (A z_k + \mu) = A (\frac{1}{2m} \sum_{k = 1}^{2m} z_k) + \mu = A \cdot 0 + \mu = \mu
\end{align*}
而covariance matrix為
\begin{align*}
    &\frac{1}{2m} \sum_{k = 1}^{2m} (x_k - \mu)(x_k - \mu)^T\\
    =\ &\frac{1}{2m} \sum_{k = 1}^{2m} ((A z_k + \mu) - \mu)((A z_k + \mu) - \mu)^T\\
    =\ &\frac{1}{2m} \sum_{k = 1}^{2m} (A z_k)(A z_k)^T = \frac{1}{2m} \sum_{k = 1}^{2m} (A z_k z_k^T A^T)\\
    =\ &A (\frac{1}{2m} \sum_{k = 1}^{2m} z_k z_k^T) A^T = A \cdot I_m \cdot A^T = A A^T = \Sigma\ \square
\end{align*}

\noindent
{\bf (c)}\\

\noindent
因為 $\Sigma$ 和 $\Phi \Phi^T$ 皆為symmetric，所以 $\Sigma$ 和 $\Phi \Phi^T$ 皆可以正交對角化。\\
令 $\Sigma$ 的eigenvalue為 $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_m$，而 $\Phi \Phi^T$ 的eigenvalue為 $\mu_1 \ge \mu_2 \ge \cdots \ge \mu_m$。\\
事實上，若 $\Phi = \left( v_1\ v_2\ \cdots\ v_k \right)$，因為 $\Phi^T \Phi = I_k$，所以 $v_1$、$v_2$、$\cdots$、$v_k$ 為 ${\mathbb R}^m$ 中的一組orthonormal vectors。\\
取 $w_1$、$w_2$、$\cdots$、$w_{m - k} \in {\mathbb R}^m$，使得 $v_1$、$v_2$、$\cdots$、$v_k$、$w_1$、$w_2$、$\cdots$、$w_{m - k}$ 構成 ${\mathbb R}^m$ 的一組orthonormal basis。\\
因為
\begin{align*}
    (\Phi \Phi^T) v_i = \Phi (\Phi^T v_i) = \Phi \cdot e_i = v_i = 1 \cdot v_i
\end{align*}
所以 $v_i$ 皆為 $\Phi \Phi^T$ 的eigenvector，且其對應的eigenvalue為1 (因此1的multiplicity為 $k$)。\\
因為
\begin{align*}
    (\Phi \Phi^T) w_i = \Phi (\Phi^T w_i) = \Phi \cdot 0 = 0 = 0 \cdot w_i
\end{align*}
所以 $w_i$ 皆為 $\Phi \Phi^T$ 的eigenvector，且其對應的eigenvalue為0 (因此0的multiplicity為 $m - k$)。\\
由此可得 $\Phi \Phi^T$ 的eigenvalue為 $\mu_1 = \mu_2 = \cdots = \mu_k = 1$，$\mu_{k + 1} = \mu_{k + 2} = \cdots = \mu_m = 0$。\\
所以
\begin{align*}
    Trace(\Phi^T \Sigma \Phi) =\ &Trace(\Phi^T (\Sigma \Phi)) = Trace((\Sigma \Phi) \Phi^T) = Trace(\Sigma (\Phi \Phi^T))\\
    \ge\ &\sum_{i = 1}^m \lambda_i \mu_{m - i + 1}\ (\href{https://en.wikipedia.org/wiki/Trace_inequality#Von_Neumann's_trace_inequality_and_related_results}{Von\ Neumann's\ Trace\ Inequality})\\
    =\ &\lambda_1 \mu_m + \lambda_2 \mu_{m - 1} + \cdots + \lambda_{m - k} \mu_{k + 1}\ +\\
    &\lambda_{m - k + 1} \mu_k + \lambda_{m - k + 2} \mu_{k - 1} + \cdots + \lambda_m \mu_1\\
    =\ &\lambda_1 \cdot 0 + \lambda_2 \cdot 0 + \cdots + \lambda_{m - k} \cdot 0 \ +\\
    &\lambda_{m - k + 1} \cdot 1 + \lambda_{m - k + 2} \cdot 1 + \cdots + \lambda_m \cdot 1\\
    =\ &\lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m
\end{align*}
因此可得 $\lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m$，即 $\Sigma$ 最小的 $k$ 個eigenvalue之和，為 $Trace(\Phi^T \Sigma \Phi)$ 的一個lower bound。\\
接著，只要證明 $\exists\ \Phi \in {\mathbb R}^{m \times k}$ 且 $\Phi^T \Phi = I_k$，使得 $Trace(\Phi^T \Sigma \Phi) = \lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m$，即可得到 $\lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m$ 為 $Trace(\Phi^T \Sigma \Phi)$ 的最小值。\\
若 $\Sigma$ 的正交對角化為 $\Sigma = Q \Lambda Q^T$，其中 $Q = \left( u_1\ u_2\ \cdots\ u_m \right)$ 為orthogonal，且 $u_i$ 所對應的eigenvalue 為 $\lambda_i$。\\
取 $\Phi = \left( u_{m - k + 1}\ u_{m - k + 2}\ \cdots\ u_m \right)$，即 $\Phi$ 的column vector為 $\Sigma$ 最小的 $k$ 個eigenvalue其對應的eigenvector。\\
則 $\Phi \in {\mathbb R}^{m \times k}$，滿足 $\Phi^T \Phi = I_k$，且
\begin{align*}
    &Trace(\Phi^T \Sigma \Phi)\\
    =\ &Trace(\Phi^T (\Sigma \Phi))\\
    =\ &Trace \left( \left(
    \begin{array}{c}
        u_{m - k + 1}^T\\
        u_{m - k + 2}^T\\
        \vdots\\
        u_m^T
    \end{array}
    \right) \cdot \Sigma \left( u_{m - k + 1}\ u_{m - k + 2}\ \cdots\ u_m \right) \right)\\
    =\ &Trace \left( \left(
    \begin{array}{c}
        u_{m - k + 1}^T\\
        u_{m - k + 2}^T\\
        \vdots\\
        u_m^T
    \end{array}
    \right) \cdot \left(\Sigma u_{m - k + 1}\ \Sigma u_{m - k + 2}\ \cdots\ \Sigma u_m \right) \right)\\
    =\ &Trace \left( \left(
    \begin{array}{c}
        u_{m - k + 1}^T\\
        u_{m - k + 2}^T\\
        \vdots\\
        u_m^T
    \end{array}
    \right) \cdot \left(\lambda_{m - k + 1} u_{m - k + 1}\ \lambda_{m - k + 2} u_{m - k + 2}\ \cdots\ \lambda_m u_m \right) \right)\\
    =\ &Trace\left( \left(
    \begin{array}{cccc}
        \lambda_{m - k + 1} \| u_{m - k + 1} \|^2 & & &\\
        & \lambda_{m - k + 2} \| u_{m - k + 2} \|^2 & &\\
        & & \ddots &\\
        & & & \lambda_m \| u_m \|^2
    \end{array}
    \right) \right)\\
    =\ &\lambda_{m - k + 1} \| u_{m - k + 1} \|^2 + \lambda_{m - k + 2} \| u_{m - k + 2} \|^2 + \cdots + \lambda_m \| u_m \|^2\\
    =\ &\lambda_{m - k + 1} \cdot 1^2 + \lambda_{m - k + 2} \cdot 1^2 + \cdots + \lambda_m \cdot 1^2\\
    =\ &\lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m
\end{align*}
故可得 $\lambda_{m - k + 1} + \lambda_{m - k + 2} + \cdots + \lambda_m$，即 $\Sigma$ 最小的 $k$ 個eigenvalue之和，為 $Trace(\Phi^T \Sigma \Phi)$ 的最小值，且當 $\Phi$ 的column vector分別為 $\lambda_{m - k + 1}$、$\lambda_{m - k + 2}$、$\cdots$、$\lambda_m$ 所對應的eigenvector時，$Trace(\Phi^T \Sigma \Phi)$ 即可取到該最小值。$\ \square$\\

\bigskip

\noindent
{\bf \Large 3. Multiclass AdaBoost (1\%)}\\

Let ${\mathcal X}$ be the input space, ${\mathcal F}$ be a collection of multiclass classifiers that map from ${\mathcal X}$ to $\llbracket 1, K \rrbracket$, where $K$ denotes the number of classes. Let $\{(x_i
, {\hat y}_i)\}^{n}_{i = 1}$ be the training data set, where $x_i \in {\mathbb R}^m$ and ${\hat y}_i \in \llbracket 1, K \rrbracket$.\\

Given $T \in {\mathbb N}$, suppose we want to find functions
\begin{align*}
    g_T^k(x) = \sum_{t = 1}^{T} \alpha_t^k f_t(x),\ k \in \llbracket 1, K \rrbracket
\end{align*}
where $f_t \in {\mathcal F}$ and $\alpha_t^k \in {\mathbb R}$ for all $t \in \llbracket 1, T \rrbracket$, $k \in \llbracket 1, K \rrbracket$, by which the
aggregated classifier $h : {\mathcal X} \rightarrow \llbracket 1, K \rrbracket$ is defined as
\begin{align*}
    h(x) = argmax_{1 \le k \le K}\ g^k_T(x)
\end{align*}

Please apply gradient boosting to show how the functions $f_t$ and coefficients $\alpha^k_t$ are computed with an aim to minimize the following loss function
\begin{align*}
    L(g_T^1, \cdots, g_T^K) = \sum_{i = 1}^{n} exp(\frac{1}{K - 1} \sum_{k \ne {\hat y}_i} g_T^k(x_i) - g_T^{{\hat y}_i}(x_i))
\end{align*}

\noindent
{\bf \large solution}\\

\begin{center}
    \includegraphics[width=\textwidth]{solution.jpg}\\
\end{center}

\end{document}